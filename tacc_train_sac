#!/bin/bash
#----------------------------------------------------
# Sample Slurm job script
#   for TACC Lonestar6 AMD Milan nodes
#
#   *** Serial Job in Normal Queue***
# 
# Last revised: October 22, 2021
#
# Notes:
#
#  -- Copy/edit this script as desired.  Launch by executing
#     "sbatch milan.serial.slurm" on a Lonestar6 login node.
#
#  -- Serial codes run on a single node (upper case N = 1).
#       A serial code ignores the value of lower case n,
#       but slurm needs a plausible value to schedule the job.
#
#  -- Use TACC's launcher utility to run multiple serial 
#       executables at the same time, execute "module load launcher" 
#       followed by "module help launcher".
#----------------------------------------------------

#SBATCH -J train_sac            # Job name
#SBATCH -o train_sac.o%j        # Name of stdout output file
#SBATCH -e train_sac.e%j        # Name of stderr error file
#SBATCH -p gpu-a100-small                 # Queue (partition) name
#SBATCH -N 1                        # Total # of nodes (must be 1 for serial)
#SBATCH -n 1                        # Total # of mpi tasks (should be 1 for serial)
#SBATCH -t 01:00:00                 # Run time (hh:mm:ss)
#SBATCH --mail-type=all             # Send email at begin and end of job
#SBATCH -A IRI24006                 # Project/Allocation name (req'd if you have more than 1)
#SBATCH --mail-user=logan.persyn@utexas.edu

# Any other commands must follow all #SBATCH directives...
# module list
# pwd
# date
source /work/09320/lpersyn/ls6/anaconda3/etc/profile.d/conda.sh
conda activate ../cs394-work-env

# Launch serial code...
python ./tianshou/atari_sac.py --logdir $SCRATCH/cs394R/final_project_logs/     # Do not use ibrun or any other MPI launcher